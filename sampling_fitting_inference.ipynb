{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madiyarzm/CS146-Bayesian-Inference/blob/main/sampling_fitting_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql2rAZkJqLqE"
      },
      "source": [
        "# Automated inference with PyMC\n",
        "\n",
        "We implement the same model as in the previous class but this time using the PyMC library. PyMC is a very powerful library with a lot of functionality and we will spend much of this course learning how to use PyMC effectively and how the algorithms behind automated inference work. We gloss over many of these details today to focus on the basic code needed for using PyMC and demonstrating that it actually does what it is supposed to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7STgJSKhqLqI"
      },
      "source": [
        "## Tasks\n",
        "\n",
        "Other than this workbook, there are no readings for today. The main goal today is to go through every line of code below in detail, to understand what each line does, and to clear up any questions you might have. We will use PyMC in almost every lesson after this one.\n",
        "\n",
        "Feel free to discuss this pre-class work with other students in the course, or attempt it on your own if you prefer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZht2ASUqLqM"
      },
      "source": [
        "## PyMC on Forum, Google Colab, and your laptop\n",
        "\n",
        "* Unfortunately, Forum doesn't provide enough computational resources to run PyMC effectively.\n",
        "* The recommended way to run the pre-class workbooks is to use Google Colab. It is free to use, has all the necessary libraries pre-installed, and is automatically saved to your Google account.\n",
        "* On your laptop, you need to install the PyMC library using `pip install pymc`. There are some more details (but not much more) in the [PyMC documentation](https://www.pymc.io/projects/docs/en/latest/installation.html). You will also need [Anaconda](https://www.anaconda.com/) already installed before installing PyMC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1Nnzq_8EqLqN"
      },
      "outputs": [],
      "source": [
        "# This is the standard naming convention for PyMC, abbreviating it to `pm`\n",
        "import pymc as pm\n",
        "\n",
        "# We still need NumPy for creating and computing with arrays\n",
        "import numpy as np\n",
        "\n",
        "# We won't use SciPy much after today, but today we use it to compare our\n",
        "# solutions (posteriors) from the previous lesson with the solutions we get\n",
        "# from PyMC.\n",
        "import scipy.stats as sts\n",
        "\n",
        "# Making plots\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLNrlah_GRnK"
      },
      "source": [
        "## Review\n",
        "\n",
        "In CS114, we covered the basics of Bayesian inference, which we assume you know for today.\n",
        "\n",
        "The core idea is that Bayesian inference combines prior beliefs about an unknown variable (which we model using a random variable) with observed data to update our uncertainty. This updating is formalized by Bayes’ rule:\n",
        "\n",
        "$$\\begin{align}\n",
        "\\text{posterior} &= \\,\\frac{\\text{likelihood} \\times \\text{prior}}{\\text{marginal likelihood}} \\\\\n",
        "P(\\theta\\,|\\,D) &= \\,\\frac{P(D \\,|\\, \\theta) \\, P(\\theta)}{P(D)}\\end{align}$$\n",
        "\n",
        "* **Prior:** Represents what we believe about a parameter ($\\theta$) before seeing data ($D$). Priors can come from past experiments, domain knowledge, past experience, or be uninformative if little is known.\n",
        "\n",
        "* **Likelihood:** Captures how probable the observed data ($D$) are under different parameter values ($\\theta$). It comes directly from the model of the data-generating process (for example, Binomial for dice rolls, Poisson for traffic counts).\n",
        "\n",
        "* **Marginal likelihood (or evidence):** Represents the total probability (marginalized over all possible parameter values, $\\theta$) of the data under this model. This is useful when comparing different models to see which model best fits the data.\n",
        "\n",
        "* **Posterior:** The updated distribution over the parameter ($\\theta$) after seeing data ($D$). It reflects both prior beliefs and additional information from the likelihood.\n",
        "\n",
        "* **Credible interval:** The posterior can be used to compute credible intervals, for example, we are 95% confident $\\theta \\in [0.14, 0.29]$. Unlike frequentist confidence intervals, a Bayesian credible interval has a direct probability meaning: given the data, there is a 95% probability the parameter lies within that interval."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBB5lkWJIswj"
      },
      "source": [
        "Describe the prior, likelihood, and posterior of the medical trial model we started in the previous class. (You can also see it below in the _Model_ section.) What is the interpretation of each of these three components — what does each component tell us about the medical trial scenario?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsqjLo6rYxvP"
      },
      "outputs": [],
      "source": [
        "# ADD YOUR ANSWER TO THE PRE-CLASS WORKBOOK ON FORUM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4IawlG7qLqP"
      },
      "source": [
        "## Data\n",
        "\n",
        "We use the same data set as in the previous lesson, focusing on the totals row of the table below.\n",
        "\n",
        "| Study          | TG improved      | TG not improved   | CG improved    | CG not improved   |\n",
        "|:-------------- | --------:| ------:| ------:| ------:|\n",
        "| Di Rienzo 2014 | 20       | 3      | 9      | 6      |\n",
        "| Galli 1994     | 10       | 6      | 11     | 7      |\n",
        "| Kaufman 1974   | 13       | 3      | 4      | 6      |\n",
        "| Qin 2014       | 35       | 10     | 21     | 18     |\n",
        "| Sanchez 2012   | 22       | 9      | 12     | 17     |\n",
        "| Silny 2006     | 7        | 3      | 0      | 10     |\n",
        "| **Totals**     | **107**  | **34** | **57** | **64** |\n",
        "\n",
        "* TG = Treatment group\n",
        "* CG = Control group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcuWc0vUqLqQ"
      },
      "outputs": [],
      "source": [
        "data = {\n",
        "    'treatment': {\n",
        "        'improved': 107,\n",
        "        'patients': 141},\n",
        "    'control': {\n",
        "        'improved': 57,\n",
        "        'patients': 121}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32gmlkXtqLqR"
      },
      "source": [
        "## Model\n",
        "\n",
        "We use the same model as in the previous lesson.\n",
        "\n",
        "**Likelihood:**\n",
        "\n",
        "$$x\\sim\\text{Binomial}(n,p)$$\n",
        "\n",
        "where $x$ and $n$ are observed variables (data) and $p$ is an unobserved variable (the parameter we want to estimate).\n",
        "\n",
        "**Prior:**\n",
        "\n",
        "$$p\\sim\\text{Beta}(1,1)$$\n",
        "\n",
        "**Posterior:**\n",
        "\n",
        "As always, the posterior PDF is proportional to the likelihood function times the prior PDF.\n",
        "\n",
        "$$f_{\\text{post}} \\propto f_{\\text{like}} \\times f_{\\text{prior}}$$\n",
        "\n",
        "$$f_{\\text{post}}(p) \\propto f_{\\text{Binomial}}(x\\,|\\,n,p) \\, f_{\\text{Beta}}(p\\,|\\,1,1)$$\n",
        "\n",
        "From the previous lesson, we already know the posterior is a Beta distribution but PyMC doesn't know that. In fact, PyMC makes no attempt to solve for the posterior distribution function directly. Instead, it approximates the posterior using simpler functions and samples from distributions. More on this below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0BgYxltqLqR"
      },
      "source": [
        "## PyMC implementation of the model\n",
        "\n",
        "Below is the PyMC code for the medical trial data set. We have to tell PyMC what the prior and likelihood functions are and how the data connect to the parameters in the likelihood function. Since the posterior is just proportional to the product of the likelihood and the prior, we don't distinguish between the prior and the likelihood explicitly in the model below. Every distribution we specify is automatically incorporated into the product prior × likelihood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1AOOQbmqLqS"
      },
      "outputs": [],
      "source": [
        "with pm.Model() as medical_model:\n",
        "    p = pm.Beta('p', alpha=1, beta=1)\n",
        "    x = pm.Binomial(\n",
        "        'x', n=data['treatment']['patients'], p=p,\n",
        "        observed=data['treatment']['improved'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDNIM4n9qLqS"
      },
      "source": [
        "Let's go through that line-by-line.\n",
        "\n",
        "**Line 1:**\n",
        "```python\n",
        "with pm.Model() as medical_model::\n",
        "```\n",
        "\n",
        "This line creates a new PyMC Model object and gives it a name, `medical_model`. We can give it any name we like — it's just a Python variable — and we will refer to the model by this name in the rest of the code. Much of the time, we'll just call the model `model`.\n",
        "\n",
        "**Line 2:**\n",
        "```python\n",
        "p = pm.Beta('p', alpha=1, beta=1)\n",
        "```\n",
        "\n",
        "This line tells PyMC that we have a variable called `p` that comes from a Beta distribution with parameters $\\alpha=1$ and $\\beta=1$. Note that the name `p` shows up twice — once as a Python variable `p = ...` and once as a string `'p'` inside the `Beta` distribution. This is strange and confusing at first and will take some getting used to.\n",
        "\n",
        "The distinction is that the string `'p'` is the name PyMC uses internally to refer to its variables and the Python variable `p` is what you use to refer to the `pm.Beta` object in your Python code.\n",
        "\n",
        "For now, just be aware that the PyMC name of a variable and the Python name of a variable are different things. We'll usually give them the same name but that is not a requirement.\n",
        "\n",
        "**Line 3:**\n",
        "```python\n",
        "x = pm.Binomial('x', n=data['treatment']['patients'], p=p, observed=data['treatment']['improved'])\n",
        "```\n",
        "\n",
        "Finally, we tell PyMC that we have another variable, $x$ that comes from a Binomial distribution with parameters $n$ and $p$. Here $n$ is a constant equal to 141, the total number of patients in the treatment group, and $p$ is the variable we defined in the previous line — the variable that has a Beta distribution.\n",
        "\n",
        "We also tell PyMC that we get to observe the value of $x$ and that its value is 107, the number of patients in the treatment group who improved.\n",
        "\n",
        "**Summary:**\n",
        "\n",
        "That defines everything we need for the model — the prior and likelihood along with the values of the observed variables (data) in the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQI0m0fVqLqT"
      },
      "source": [
        "## Visualize the model\n",
        "\n",
        "Now that we've defined the model, we can ask PyMC to represent it as a directed graph. The graph shows which variables depend on which other variables. This visualization is useful for checking that you specified your model correctly.\n",
        "\n",
        "(This line of code won't work on your laptop unless you have the [Graphviz](https://graphviz.org/) package installed. It's not a Python package and you need to install it through your operating system. It's installed on Google Colab.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAh17DvFqLqi"
      },
      "outputs": [],
      "source": [
        "pm.model_to_graphviz(medical_model)\n",
        "\n",
        "# The two lines below are not needed unless you are on Forum. For\n",
        "# some reason, it can't display the PDF generated in the line above.\n",
        "from IPython.display import Image\n",
        "Image(pm.model_to_graphviz(medical_model).render(format='png'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYNEP4_cqLqk"
      },
      "source": [
        "## Compute the posterior distribution\n",
        "\n",
        "PyMC provides two classes of algorithms for approximating the posterior distribution of a model, namely sampling (Markov chain Monte Carlo algorithms) and fitting (variational inference algorithms). We will use both types of algorithms today to get an idea of how they work and explore each type of algorithm in more depth later in the course."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDRYWQMQqLqk"
      },
      "source": [
        "### Approximation 1: Inference by sampling\n",
        "\n",
        "Sampling algorithms are able to generate random samples from the posterior distribution without actually being able to compute the analytical form (equation) of the posterior PDF.\n",
        "\n",
        "In theory, sampling algorithms generate samples that really do come from the posterior distribution, _if you give them enough time to run_. Unfortunately, the theory requires \"enough time\" to mean \"forever\" before you are guaranteed that the samples really come from the distribution you want.\n",
        "\n",
        "In practice, the samples are _approximately_ from the correct distribution and the approximation is usually really good. We have some metrics we can monitor to sanity check whether the sampler is behaving as it should but things can go wrong and later we will learn what problems to look out for.\n",
        "\n",
        "The main drawback of sampling algorithms is that they are slow since they have to run approximately forever to generate accurate samples. The code cell below will take a while to run. Sampling from a PyMC model will feel slow even for simple models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bygHoL_PqLqk"
      },
      "outputs": [],
      "source": [
        "with medical_model:\n",
        "    inference = pm.sample()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jyx2mwQqLql"
      },
      "source": [
        "**Line 1:**\n",
        "```python\n",
        "with medical_model:\n",
        "```\n",
        "\n",
        "This line reactivates the model we defined earlier so that Line 2 below which model to sample from. We can define multiple models in one notebook and use this `with` line to activate a particular model.\n",
        "\n",
        "**Line 2:**\n",
        "```python\n",
        "inference = pm.sample()\n",
        "```\n",
        "\n",
        "This line runs the default sampler (called the No-U-Turn Sampler, or NUTS) and stores the results in the variable called `inference`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zu9DOhnqLql"
      },
      "outputs": [],
      "source": [
        "inference.posterior"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v5CTZaQqLql"
      },
      "source": [
        "We don't normally print out the posterior like this but it is helpful to look at it at least once. The `inference.posterior` object contains samples for each of the unknown variables in the model — in this case, that's just the variable $p$. In more complicated models, we will see more variables listed here.\n",
        "\n",
        "Note that there are multiple \"chains\" and each chain has multiple \"draws\". We will learn more about what chains are later in the course. For now, just know that we should use all the values (\"draws\" — people often talk about drawing a sample from a distribution, hence the name) in all the chains to represent the posterior.\n",
        "\n",
        "Let's get all those values for $p$ into one array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-f82wTi4qLql"
      },
      "outputs": [],
      "source": [
        "all_p_samples = inference.posterior.p.values.flatten()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW7HyDn1qLqm"
      },
      "source": [
        "You should be able to read that line. We get all the values for the $p$ variable from the posterior distribution stored in the `inference` object. The `flatten()` function turns the 2-dimensional array of values (it's a \"chains\" by \"draws\" array) into a 1-dimensional array of values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agbdDYcoqLqm"
      },
      "outputs": [],
      "source": [
        "# Plot the samples from the approximate posterior as well as the exact solution\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.title('Comparison of the sampling approximation and the exact posterior')\n",
        "plt.xlabel('p')\n",
        "plt.ylabel('probability density')\n",
        "plt.hist(\n",
        "    all_p_samples, bins=20, density=True,\n",
        "    edgecolor='white', label='approximation')\n",
        "x = np.linspace(0.5, 0.9, 500)\n",
        "y = sts.beta.pdf(x, 108, 35)\n",
        "plt.plot(x, y, label='exact')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMSlciU7qLqm"
      },
      "source": [
        "If you already know how to use Matplotlib to plot functions and histograms, you can skip this part. If not, read on!\n",
        "\n",
        "We ran `import matplotlib.pyplot as plt` earlier in this notebook. So, every time we refer to `plt` below it is a reference to the Matplotlib library.\n",
        "\n",
        "**Lines 2–5:**\n",
        "```python\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.title('Comparison of the sampling approximation and the exact posterior')\n",
        "plt.xlabel('p')\n",
        "plt.ylabel('probability density')\n",
        "```\n",
        "\n",
        "These lines create a new, empty figure with a particular size (optional, you don't have to specify a size), give the figure a title, and label the $x$ (horizontal) and $y$ (vertical) axes. **You should always provide titles and labels for your plots.** This is important for making interpretable plots. You will be graded on the #Professionalism LO for your plots.\n",
        "\n",
        "**Lines 6–8:**\n",
        "```python\n",
        "plt.hist(all_p_samples, bins=20, density=True, edgecolor='white', label='approximation')\n",
        "```\n",
        "\n",
        "These lines produce a histogram. The `hist()` function will look at the values inside the list or array you provide as the first input and figure out how to bin the data to create the histogram.\n",
        "* If you want to, you can tell it how many bins you want (we use `bins=20` here) but that is optional.\n",
        "* The `density=True` argument tells Matplotlib that we want to change the vertical scale of the histogram so that it behaves like a probability density function. This means if you add up the areas (width × height) of all the bars, the total area will be 1 just like a PDF. By default, with `density=False`, the histogram shows counts on the vertical axis — that is, the number of values from the array that fall inside each bin.\n",
        "* The bars look prettier with white edges, so we have `edgecolor='white'` in there. We can change the color of anything on the plot if we want to. Why do we bother doing this? Because it makes the plot more readable. #Professionalism\n",
        "* And we give the histogram a label with `label='approximation'`. This is important because there are two plots on that figure — the histogram and the curve of the posterior Beta PDF. We identify them on the plot using labels. #Professionalism\n",
        "\n",
        "**Lines 9–11:**\n",
        "```python\n",
        "x = np.linspace(0.5, 0.9, 500)\n",
        "y = sts.beta.pdf(x, 108, 35)\n",
        "plt.plot(x, y, label='exact')\n",
        "```\n",
        "\n",
        "These lines produce the curved line. To make line plots, we provide Matplotlib with an array of $x$ coordinates and an array of $y$ coordinates as the first two arguments to the `plot()` function. Matplotlib then draws a line through those coordinates.\n",
        "\n",
        "Line 9 creates an array of 500 evenly-spaced $x$-coordinates starting at 0.5 on the left and ending at 0.9 on the right. You can see that the orange curve runs from 0.5 to 0.9 on the plot. You can choose to use more or fewer than 500 points. If you use too few, the plot won't look smooth since Matplotlib just connects the $(x,y)$-coordinates with straight line segments. If you use too many, the plot will take longer to produce. A few hundred points are usually fine.\n",
        "\n",
        "Line 10 creates an array with 500 values containing one $y$-value for each $x$-value, where the $y$-values are computed using the posterior Beta PDF for the treatment group. In this case study, the model is simple enough that we can compute the exact posterior distribution mathematically. We're doing that to check how well the PyMC approximation is doing. In real case studies, we won't be able to compute the posterior mathematically and we won't be able to compare the PyMC approximation to anything like we're doing here.\n",
        "\n",
        "Line 11 adds the curved line to the same figure, on top of the histogram that is there already. We provide the $x$ and $y$-coordinates and a label.\n",
        "\n",
        "**Line 12:**\n",
        "```python\n",
        "plt.legend()\n",
        "```\n",
        "\n",
        "This line turns on the legend. The legend is the box in the top-right corner that contains all the plot labels. If you forget this line, the legend won't appear even if you labeled all your plots.\n",
        "\n",
        "**Line 13:**\n",
        "```python\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "This line displays the plot in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfoyUX5NqLqm"
      },
      "source": [
        "### Approximation 2: Inference by fitting a function\n",
        "\n",
        "The basic idea here is that we approximate a complicated function (the posterior) using a simple function. Here, \"simple\" means it should be easy to generate samples from the approximation and it should be easy to evaluate the PDF of the approximation. For example, PyMC might fit a Normal PDF to the true posterior distribution. The approximation won't be perfect (unless the posterior really is a Normal distribution!) but we have existing algorithms for evaluating the Normal PDF and generating samples from a Normal random variable.\n",
        "\n",
        "The main advantage of fitting algorithms is that they are fast — much faster than sampling algorithms. This has the benefit that you can iterate much faster on improving your model since you don't have to wait for the inference algorithm to complete. With faster algorithms, you can also generally handle larger data sets in a reasonable amount of time.\n",
        "\n",
        "The main drawback of fitting algorithms is that we know the answer they produce is wrong — by design. The approximate posterior distribution is just that — an approximation. Samplers have the theoretical guarantee that if we run them for long enough (approximately forever), they will generate samples from the true posterior distribution. Fitting algorithms can never achieve this.\n",
        "\n",
        "In short, if you want a decent solution fast, use function fitting; if you want an accurate solution and are willing to wait, use sampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w55K_uifqLqn"
      },
      "outputs": [],
      "source": [
        "with medical_model:\n",
        "    approx = pm.fit()\n",
        "    inference = approx.sample(4000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-5k1DVBqLqn"
      },
      "source": [
        "**Line 1:**\n",
        "```python\n",
        "with medical_model:\n",
        "```\n",
        "\n",
        "Activate the model.\n",
        "\n",
        "**Line 2:**\n",
        "```python\n",
        "approx = pm.fit()\n",
        "```\n",
        "\n",
        "This line runs the default function fitting algorithm (the Automatic Differentiation Variational Inference algorithm, or \"ADVI\") to fit an approximating PDF to the true posterior distribution.\n",
        "\n",
        "**Line 3:**\n",
        "```python\n",
        "inference = approx.sample(4000)\n",
        "```\n",
        "\n",
        "This line generates 4000 samples from the approximating distribution. This is fast since the approximating distribution is simple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJTVSLT9qLqn"
      },
      "outputs": [],
      "source": [
        "# Plot the samples from the function fitting posterior as well as the exact solution\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.title('Comparison of the function fitting approximation and the exact posterior')\n",
        "plt.xlabel('p')\n",
        "plt.ylabel('probability density')\n",
        "plt.hist(\n",
        "    inference.posterior.p.values.flatten(), bins=20, density=True,\n",
        "    edgecolor='white', label='approximation')\n",
        "x = np.linspace(0.5, 0.9, 500)\n",
        "y = sts.beta.pdf(x, 108, 35)\n",
        "plt.plot(x, y, label='exact')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0ZmsdljqLqo"
      },
      "source": [
        "## Task\n",
        "\n",
        "* Reuse (copy-and-paste) the code above to calculate the sampling and function fitting approximations for the **control group**.\n",
        "* Plot the approximations and histograms and the true Beta() posterior as a curve like we did above. Check how good the approximations are.\n",
        "* Remember to add your questions or topics for class discussion to the top of this workbook!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jecm5k_rqLqq"
      },
      "outputs": [],
      "source": [
        "# YOUR WORK GOES HERE
        import pymc as pm
        import matplotlib.pyplot as plt
        import numpy as np
        import scipy.stats as sts
        
        test_data = {
            'treatment': {
                'improved': 107,
                'patients': 141},
            'control': {
                'improved': 57,
                'patients': 121}
        }
        
        with pm.Model() as my_model:
          p = pm.Beta('p',alpha = 1, beta = 1)
          x = pm.Binomial('x', n = test_data['control']['patients'], p = p, observed = test_data['control']['improved'])
        
        pm.model_to_graphviz(my_model)
        
        with my_model:
          inference1 = pm.sample()
        
        all_p_samples_1 = inference1.posterior.p.values.flatten()
        
        plt.figure(figsize=(8, 4))
        plt.title('Comparison of the sampling approximation and the exact posterior')
        plt.xlabel('p')
        plt.ylabel('probability density')
        plt.hist(all_p_samples_1, bins=20, density=True, edgecolor='white', label='approximation')
        x = np.linspace(0.5, 0.9, 500)
        y = sts.beta.pdf(x, 58, 65)
        plt.plot(x, y, label='exact')
        plt.legend()
        plt.show()
        
        with my_model:
            approx = pm.fit()
            inference2 = approx.sample(4000)
        
        all_p_samples_fit = inference2.posterior.p.values.flatten()
        
        plt.figure(figsize=(8,4))
        plt.title('Function fitting approximation vs exact posterior')
        plt.xlabel('p')
        plt.ylabel('probability density')
        plt.hist(all_p_samples_fit, bins=20, density=True, edgecolor='white', label='approximation')
        x = np.linspace(0,1,500)
        y = sts.beta.pdf(x, 58, 65)
        plt.plot(x, y, label='exact')
        plt.legend()
        plt.show()
        
        
        
        # PASTE THIS CODE INTO THE FORUM PRE-CLASS WORKBOOK WHEN YOU'RE DONE!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvKPaFqQqLqr"
      },
      "source": [
        "## (Optional) Further reading\n",
        "\n",
        "We used only the Binomial and Beta distributions today but PyMC has many, many distributions predefined. You can view them in [the PyMC documentation](https://www.pymc.io/projects/docs/en/stable/api/distributions.html). Reading this documentation is optional for now. We will cover many of those distributions as we progress through the course. However, if you would like to get started building your own models, the PyMC documentation is an invaluable resource.\n",
        "\n",
        "The PyMC documentation also contains lots of [example notebooks](https://www.pymc.io/projects/examples/en/latest/gallery.html) that you can use to see how more complicated data analyses are done. Feel free to read ahead if you want to. Most of these notebooks are much more complicated than what we have done so far."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
